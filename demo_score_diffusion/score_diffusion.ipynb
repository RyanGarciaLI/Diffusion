{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define the U-net based Time-dependent Score Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "class TimeEncoding(nn.Module):\n",
    "    \"\"\" Fourier enconding to time. \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, scale=30.0) -> None:\n",
    "        super().__init__()\n",
    "        # randomly sample weights during initialization. These weights are fixed\n",
    "        # during optimization and are not trainable.\n",
    "        # randn: random normal noise\n",
    "        # half for sin, half for cos\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expand dimension: x[:, None], W[None, :]\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi   # TODO: syntatic sugar!!!!\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "    \n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\" A fully connected layer that reshapes outputs to feature maps. \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None]  # expand dimensions\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\" Time-dependent score estimiate model based on U-Net. \"\"\"\n",
    "\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256) -> None:\n",
    "        \"\"\" Initialize a time-dependent score-based network. \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.embed = nn.Sequential(TimeEncoding(embed_dim=embed_dim), nn.Linear(embed_dim, embed_dim))\n",
    "\n",
    "        # U-net encoder, space decreases, channel increases.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "\n",
    "        # U-net decoder, space increases, channel decreases, including skip connections\n",
    "\n",
    "        self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense5 = Dense(embed_dim, channels[2])\n",
    "        self.tgnorm5 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "\n",
    "        self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n",
    "        self.dense6 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm6 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
    "        self.dense7 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm7 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0]+ channels[0], 1, 3, stride=1)\n",
    "\n",
    "        self.act = lambda x: x * torch.sigmoid(x)   # swish activation function\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        embed = self.act(self.embed(t))\n",
    "\n",
    "        # encoder\n",
    "        h1 = self.conv1(x)\n",
    "\n",
    "        h1 += self.dense1(embed)    # inject time t\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "\n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "\n",
    "        h4 = self.conv4(h3)\n",
    "        h4 += self.dense4(embed)\n",
    "        h4 = self.gnorm4(h4)\n",
    "        h4 = self.act(h4)\n",
    "\n",
    "        # decoder\n",
    "        h = self.tconv4(h4)\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm5(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        h = self.tconv3(torch.cat([h, h3], dim=1)) # skip connection\n",
    "        h += self.dense6(embed)\n",
    "        h = self.tgnorm6(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "        h += self.dense7(embed)\n",
    "        h = self.tgnorm7(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "        # Normalize output\n",
    "        # divide the expectation of second order norm\n",
    "        # equivalent to moving lambda into score net\n",
    "        # objective: make predict score's 2nd order norm approachs real score's 2nd order norm\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "\n",
    "        return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define SDE and Denoising Score Matching Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dx = sigma^t * dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "def marginal_prob_std(t, sigma):\n",
    "    \"\"\" Compute standard deviation at any time t. \"\"\"\n",
    "\n",
    "    t = torch.tensor(t, device=device)\n",
    "    return torch.sqrt((sigma**(2*t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "    \"\"\" Compute diffuison coefficient at any time t. Note that there is no draft coefficient in this demo. \"\"\"\n",
    "\n",
    "    return torch.tensor(sigma**t, device=device)\n",
    "\n",
    "\n",
    "sigma = 25.0\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)    # non-parameteric function\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)        # non-parameteric function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(score_model:nn.modules, x:torch.tensor, marginal_prob_std, eps=1e-5):\n",
    "    \"\"\" The loss function for training score-based gnerative models. \n",
    "    \n",
    "    Args:\n",
    "        model: A PyTorch model instance that represents a time-dependent score-based model.\n",
    "        x: A mini-batch of training data.\n",
    "        marginal_prob_std: A function that gives the standard deviation of the perturbation kernel.\n",
    "        eps: A tolerance value for numerical stability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: randomly generate [batch_size] float time t from [0.00001, 0.99999], Uniform\n",
    "    # x.shape[0] = batch_size = 32, each batch uses the same time step.\n",
    "    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps # [batch_size x 1]\n",
    "\n",
    "    # Step 2: get perturbed_x: sample P_t(x) based on reparameterization\n",
    "    z = torch.randn_like(x)\n",
    "    std = marginal_prob_std(random_t)\n",
    "    perturbed_x = x + z * std[:, None, None, None]\n",
    "\n",
    "    # Step 3: put perturbed sample and time into Score Network to predict score\n",
    "    score = score_model(perturbed_x, random_t)\n",
    "\n",
    "    # Step 4: Compute score matching loss\n",
    "    loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1,2,3))) #TODO: why +\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class EMA(nn.Module):\n",
    "    def __init__(self, model, decay=0.9999, device=None):\n",
    "        super(EMA, self).__init__()\n",
    "        # make a copy of the model for accumulating moveing average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e,m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Socre-based model on MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import tqdm\n",
    "\n",
    "score_model = torch.nn.DataParallel(ScoreNet(marginal_prob_std=marginal_prob_std_fn))\n",
    "score_model = score_model.to(device)\n",
    "\n",
    "n_epochs = 50 #@param {'type': 'integer'}\n",
    "## size of a mini-batch\n",
    "batch_size = 32 #@param {'type': 'integer'}\n",
    "## learning rate\n",
    "lr = 1e-4 #@param {'type' : 'number'}\n",
    "\n",
    "dataset = MNIST('.', train=True, transform=transforms.ToTensor(), download=True) # gray scale images\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "optimizer = Adam(score_model.parameters(), lr=lr)\n",
    "tqdm_epoch = tqdm.tqdm(range(n_epochs))\n",
    "\n",
    "ema = EMA(score_model)\n",
    "for epoch in tqdm_epoch:\n",
    "    # training speed: Mac CPU: 250s/Epoch, Kaggle GPU: 30s/Epoch, Colab: 35s/Epoch\n",
    "    avg_loss = 0\n",
    "    num_items = 0\n",
    "    for x, y in data_loader:\n",
    "        # y in [0,9], label\n",
    "        x = x.to(device) # [32 x 1 x 28 x 28] [Batch_size x Channel x Width x Height]\n",
    "        loss = loss_fn(score_model, x, marginal_prob_std_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update(score_model)\n",
    "        avg_loss += loss.item() * x.shape[0] # what is item()\n",
    "        num_items += x.shape[0]\n",
    "\n",
    "print('Average ScoreMatching Loss: {:5f}'.format(avg_loss / num_items))\n",
    "torch.save(score_model.state_dict(), f'ckpt_{n_epochs}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The number of sampling steps\n",
    "num_steps = 500\n",
    "\n",
    "def euler_sampler(score_model, \n",
    "                  marginal_prob_std, \n",
    "                  diffusion_coeff, \n",
    "                  batch_size=64, \n",
    "                  num_steps=num_steps, \n",
    "                  device='cuda', \n",
    "                  eps=1e-3):\n",
    "\n",
    "    # Step1: define initial time t and random sample from prior distribution \n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    init_x = torch.randn(batch_size, 1, 28, 28, device=device) \\\n",
    "                * marginal_prob_std(t)[:, None, None, None]\n",
    "    \n",
    "    # Step2: define sampling reverse-time grid and time step\n",
    "    time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "    step_size = time_steps[0] - time_steps[1]\n",
    "\n",
    "    # Step3: solve reverse-time SDE using Euler Algo\n",
    "    x = init_x\n",
    "    with torch.no_grad():\n",
    "        for time_step in tqdm.tqdm(time_step):\n",
    "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "            g = diffusion_coeff(batch_time_step)\n",
    "            mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step) * step_size\n",
    "            x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)\n",
    "\n",
    "    return mean_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euler + Langvein Dynamic Sampling (Predictor-Corrector Sampler) to generate higher quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## signal to noise ratio\n",
    "SNR = 0.16\n",
    "num_steps = 500\n",
    "langevin_steps = 10\n",
    "\n",
    "def pc_sampler(score_model, \n",
    "               marginal_prob_std, \n",
    "               diffusion_coeff, \n",
    "               batch_size=64, \n",
    "               num_steps=num_steps,\n",
    "               corrector_steps=10,\n",
    "               snr=SNR, \n",
    "               device='cuda', \n",
    "               eps=1e-3):\n",
    "    \"\"\" Generate samples from score-based models with Predictor-Corrector method. \n",
    "    \n",
    "    Args:\n",
    "        score_model: A PyTorch model instance that represents a time-dependent score-based model.\n",
    "        marginal_prob_std: A function that gives the standard deviation of the perturbation kernel.\n",
    "        diffusion_coeff: A function that gives the diffusion coefficient of SDE\n",
    "        batch_size: The number of samplers to generate by calling this function once.\n",
    "        num_steps: The number of sampling steps. Equivalent to the number of discretized time steps. \n",
    "        corrector_steps: The number of langevin MCMC steps.\n",
    "        snr: signal to noise ratio.\n",
    "        device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "        eps: A tolerance value for numerical stability. \n",
    "\n",
    "    Returns:\n",
    "        Samples.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step1: define initial time t and random sample from prior distribution \n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    init_x = torch.randn(batch_size, 1, 28, 28, device=device) \\\n",
    "                * marginal_prob_std(t)[:, None, None, None]\n",
    "    \n",
    "    # Step2: define sampling reverse-time grid and time step\n",
    "    time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "    step_size = time_steps[0] - time_steps[1]\n",
    "\n",
    "    # Step3: alter langevin sampling and Euler Algo\n",
    "    x = init_x\n",
    "    with torch.no_grad():\n",
    "        for time_step in tqdm.tqdm(time_step):\n",
    "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "\n",
    "            # Corrector step (Langevin MCMC)\n",
    "            grad = score_model(x, batch_time_step)\n",
    "            grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n",
    "            noise_norm = np.sqrt(np.prod(x.shape[1:]))\n",
    "            langevin_step_size = 2 * (snr * noise_norm / grad_norm)**2\n",
    "            print(f\"{langevin_step_size=}\") #TODO: typo?\n",
    "\n",
    "            for _ in range(corrector_steps):\n",
    "                x = x + langevin_step_size * grad + torch.sqrt(2 * langevin_step_size) * torch.rand_like(x)\n",
    "                grad = score_model(x, batch_time_step)\n",
    "                grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean() # reshape(d,-1) : infer the last dimension\n",
    "                noise_norm = np.sqrt(np.prod(x.shape[1:]))\n",
    "                langevin_step_size = 2 * (snr * noise_norm / grad_norm) ** 2\n",
    "                print(f\"{langevin_step_size}\")\n",
    "\n",
    "            # predictor step (Euler-Maruyama)\n",
    "            g = diffusion_coeff(batch_time_step)\n",
    "            mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step) * step_size\n",
    "            x = mean_x + torch.sqrt(g**2 * step_size)[:, None, None, None] * torch.randn_like(x)\n",
    "\n",
    "    return mean_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orinary Differential Equation\n",
    "\n",
    "\\begin{align*}\n",
    "d \\mathbf(x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "## The error tolerance for the black-box ODE solver\n",
    "error_tolerance = 1e-5 \n",
    "\n",
    "def ode_sampler(score_model, \n",
    "                marginal_prob_std,\n",
    "                diffusion_coeff,\n",
    "                batch_size=64,\n",
    "                atol=error_tolerance,\n",
    "                rtol=error_tolerance,\n",
    "                device='cuda',\n",
    "                z=None,\n",
    "                eps=1e-3\n",
    "                ):\n",
    "    \n",
    "    # Step1 define initial time and x\n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    if z is None:\n",
    "        init_x = torch.randn(batch_size, 1, 28, 28, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
    "    else:\n",
    "        init_x = z\n",
    "\n",
    "    shape = init_x.shape\n",
    "\n",
    "    # Step2: define score prediction function and ordinary differential function\n",
    "    def score_eval_wrapper(sample, time_steps):\n",
    "        \"\"\" A wrapper of the score-based model for use by the ODE solver. \"\"\"\n",
    "\n",
    "        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape(sample.shape[0])\n",
    "        with torch.no_grad():\n",
    "            score = score_model(sample, time_steps)\n",
    "        return score.cpu().numpy().reshape((-1,)).astype(np.float64)\n",
    "    \n",
    "\n",
    "    def ode_func(t, x):\n",
    "        \"\"\" The ODE function for use by the ODE solver. \"\"\"\n",
    "\n",
    "        time_steps = np.ones((shape[0],)) * t\n",
    "        g = diffusion_coeff(torch.tensor(t)).cpu().numpy()\n",
    "        return -0.5 * (g**2) * score_eval_wrapper(x, time_steps)\n",
    "    \n",
    "\n",
    "    # Step 3 call ODE solver to compute the predict sample at time t = eps\n",
    "    res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')\n",
    "    print(f\"Number of function evaluation: {res.nfev}\")\n",
    "\n",
    "    x = torch.tensor(res.y[:, -1], device=device).reshape(shape)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import trained model and compare different sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "\n",
    "## load the pre-trained checkpoint from disk\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "ckpt = torch.load('ckpt.pth', map_location=device)\n",
    "score_model.load_state_dict(ckpt)\n",
    "\n",
    "sample_batch_size = 64\n",
    "sampler = pc_sampler # pc_sampler, euler_sampler or ode_sampler\n",
    "\n",
    "t1 = time.time()\n",
    "## Generate samples using the specified sampler.\n",
    "samples = sampler(score_model, marginal_prob_std_fn, diffusion_coeff_fn, sample_batch_size, device=device)\n",
    "\n",
    "t2 = time.time()\n",
    "print(f\"{str(sampler)} sampling costs {t2-t1}s\")\n",
    "\n",
    "## sample visualization\n",
    "samples = samples.clamp(0.0, 1.0)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "sample_grid = make_grid(samples, nrow=int(np.sqrt(sample_batch_size)))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(sample_grid.permute(1,2,0).cpu(), vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score-based diffusion model on MNIST experiemnt results.\n",
    "\n",
    "Train 50 epochs, loss decreases to 16\n",
    "\n",
    "ODE sampling: fast, low quality\n",
    "\n",
    "Euler sampling: slow, middle quality\n",
    "\n",
    "PC sampling: slowest, high quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEGSDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
